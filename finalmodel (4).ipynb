{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c99fba0-678f-4abb-82bd-6262c2897de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in ./.local/lib/python3.10/site-packages (0.20.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.local/lib/python3.10/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.local/lib/python3.10/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.local/lib/python3.10/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.local/lib/python3.10/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.local/lib/python3.10/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: triton==3.1.0 in ./.local/lib/python3.10/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.local/lib/python3.10/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.local/lib/python3.10/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.local/lib/python3.10/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.local/lib/python3.10/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.local/lib/python3.10/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in ./.local/lib/python3.10/site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2->torch) (3.0.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9a5f9a-a362-4501-8ab8-9e3021fbad0d",
   "metadata": {},
   "source": [
    "### Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b94c295a-631b-4b2e-99bc-4758764401d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_93/3188550002.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_images = torch.load(\"final_train_images.pt\", map_location=torch.device('cpu'))\n",
      "/tmp/ipykernel_93/3188550002.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_labels = torch.load(\"final_train_labels.pt\", map_location=torch.device('cpu'))\n",
      "/tmp/ipykernel_93/3188550002.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  valid_images = torch.load(\"final_valid_images.pt\", map_location=torch.device('cpu'))\n",
      "/tmp/ipykernel_93/3188550002.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  valid_labels = torch.load(\"final_valid_labels.pt\", map_location=torch.device('cpu'))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "train_images = torch.load(\"final_train_images.pt\", map_location=torch.device('cpu'))\n",
    "train_labels = torch.load(\"final_train_labels.pt\", map_location=torch.device('cpu'))\n",
    "valid_images = torch.load(\"final_valid_images.pt\", map_location=torch.device('cpu'))\n",
    "valid_labels = torch.load(\"final_valid_labels.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af7b88-1cf2-4110-ba66-71f7076fd030",
   "metadata": {},
   "source": [
    "# With TDA(Persistent Landscape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07e835b1-56d8-422d-b055-7d7d27718296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/elicer/wandb/run-20241129_181158-b9p6bdyr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uus0314-korea-university/COSE474/runs/b9p6bdyr' target=\"_blank\">PL_CNN+TDA(Epoch 5)</a></strong> to <a href='https://wandb.ai/uus0314-korea-university/COSE474' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uus0314-korea-university/COSE474' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uus0314-korea-university/COSE474/runs/b9p6bdyr' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474/runs/b9p6bdyr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 6.9545, Accuracy: 0.5350\n",
      "Epoch [2/5], Loss: 0.5615, Accuracy: 0.6783\n",
      "Epoch [3/5], Loss: 0.4619, Accuracy: 0.7633\n",
      "Epoch [4/5], Loss: 0.3696, Accuracy: 0.8250\n",
      "Epoch [5/5], Loss: 0.3006, Accuracy: 0.8533\n",
      "Validation Loss: 0.7560, Accuracy: 0.6333\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁▄▆▇█</td></tr><tr><td>Train Loss</td><td>█▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁</td></tr><tr><td>Validation Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>0.85333</td></tr><tr><td>Train Loss</td><td>0.30063</td></tr><tr><td>Validation Accuracy</td><td>0.63333</td></tr><tr><td>Validation Loss</td><td>0.75596</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PL_CNN+TDA(Epoch 5)</strong> at: <a href='https://wandb.ai/uus0314-korea-university/COSE474/runs/b9p6bdyr' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474/runs/b9p6bdyr</a><br/> View project at: <a href='https://wandb.ai/uus0314-korea-university/COSE474' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 150 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241129_181158-b9p6bdyr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gudhi as gd\n",
    "from gudhi import representations\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def compute_persistent_landscape(image_tensor, num_landscapes=2, resolution=100):\n",
    "    \"\"\"\n",
    "    이미지 텐서를 입력받아 Persistent Landscape를 계산.\n",
    "    \"\"\"\n",
    "    image_np = image_tensor.permute(1, 2, 0).numpy()\n",
    "    resized_image = cv2.resize(image_np, (128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    try:\n",
    "\n",
    "        cubical_complex = gd.CubicalComplex(top_dimensional_cells=resized_image)\n",
    "        persistence_diagram = cubical_complex.persistence()\n",
    "        h0 = cubical_complex.persistence_intervals_in_dimension(0)\n",
    "        h1 = cubical_complex.persistence_intervals_in_dimension(1)\n",
    "\n",
    "\n",
    "        def remove_infinite_intervals(intervals):\n",
    "            return intervals[np.isfinite(intervals[:, 1])] if len(intervals) > 0 else intervals\n",
    "\n",
    "        h0 = remove_infinite_intervals(h0)\n",
    "        h1 = remove_infinite_intervals(h1)\n",
    "\n",
    "\n",
    "        landscape_generator = representations.Landscape(num_landscapes=num_landscapes, resolution=resolution)\n",
    "        filtered_h0 = np.array(h0) if len(h0) > 0 else np.empty((0, 2))\n",
    "        filtered_h1 = np.array(h1) if len(h1) > 0 else np.empty((0, 2))\n",
    "\n",
    "\n",
    "        landscape_h0 = landscape_generator.fit_transform([filtered_h0]).flatten() if len(filtered_h0) > 0 else np.zeros(num_landscapes * resolution)\n",
    "        landscape_h1 = landscape_generator.fit_transform([filtered_h1]).flatten() if len(filtered_h1) > 0 else np.zeros(num_landscapes * resolution)\n",
    "\n",
    "\n",
    "        combined_feature = np.concatenate([landscape_h0, landscape_h1])\n",
    "        return torch.from_numpy(combined_feature).float()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"TDA feature computation error: {e}\")\n",
    "        return torch.zeros(num_landscapes * resolution * 2)\n",
    "\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, images, labels, num_landscapes=5, resolution=100):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.num_landscapes = num_landscapes\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        tda_features = compute_persistent_landscape(image, num_landscapes=self.num_landscapes, resolution=self.resolution)\n",
    "        return image, label, tda_features\n",
    "\n",
    "\n",
    "class CNNWithTDA(nn.Module):\n",
    "    def __init__(self, num_classes=2, tda_feature_dim=200, reduced_dim=1024):\n",
    "        super(CNNWithTDA, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.feature_reduce = nn.Linear(262144, reduced_dim)  # CNN 차원 축소\n",
    "        self.fc1 = nn.Linear(reduced_dim + tda_feature_dim, 128)  # 결합된 피처 처리\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x, tda_features):\n",
    "        cnn_features = self.cnn(x)\n",
    "        cnn_features = self.feature_reduce(cnn_features)  # 차원 축소\n",
    "        combined_features = torch.cat((cnn_features, tda_features), dim=1)  # 결합\n",
    "        x = self.fc1(combined_features)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, train_loader, device, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for images, labels, tda_features in train_loader:\n",
    "            images, labels, tda_features = images.to(device), labels.to(device), tda_features.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, tda_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_accuracy})\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_and_visualize(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels, tda_features) in enumerate(valid_loader):\n",
    "            images, labels, tda_features = images.to(device), labels.to(device), tda_features.to(device)\n",
    "            outputs = model(images, tda_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "\n",
    "            for img_idx in range(images.size(0)):\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.imshow(images[img_idx].cpu().squeeze(), cmap=\"gray\")\n",
    "                title = f\"GT: {labels[img_idx].item()}, Pred: {predicted[img_idx].item()}\"\n",
    "                ax.set_title(title)\n",
    "                ax.axis(\"off\")\n",
    "                wandb.log({f\"Sample Prediction (Batch {batch_idx}, Image {img_idx})\": wandb.Image(fig)})\n",
    "                plt.close(fig)\n",
    "\n",
    "    valid_loss = running_loss / len(valid_loader)\n",
    "    valid_accuracy = correct / total\n",
    "    wandb.log({\"Validation Loss\": valid_loss, \"Validation Accuracy\": valid_accuracy})\n",
    "    print(f\"Validation Loss: {valid_loss:.4f}, Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "wandb.init(project=\"COSE474\", name=\"PL_CNN+TDA(Epoch 5)\")\n",
    "wandb.config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_epochs\": 5,\n",
    "    \"num_classes\": 2,\n",
    "    \"tda_feature_dim\": 400\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "num_landscapes = 2\n",
    "resolution = 100\n",
    "tda_feature_dim = num_landscapes * resolution * 2\n",
    "\n",
    "train_dataset = MRIDataset(train_images, train_labels, num_landscapes=num_landscapes, resolution=resolution)\n",
    "valid_dataset = MRIDataset(valid_images, valid_labels, num_landscapes=num_landscapes, resolution=resolution)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNWithTDA(num_classes=wandb.config[\"num_classes\"], tda_feature_dim=tda_feature_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=wandb.config[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train(model, train_loader, device, optimizer, criterion, num_epochs=wandb.config[\"num_epochs\"])\n",
    "\n",
    "\n",
    "evaluate_and_visualize(model, valid_loader, device)\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d631d6ef-7bcf-47ec-9252-ea5333bbccfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">PL2_CNN+TDA(Epoch 10)</strong> at: <a href='https://wandb.ai/uus0314-korea-university/COSE474/runs/5ni41cm3' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474/runs/5ni41cm3</a><br/> View project at: <a href='https://wandb.ai/uus0314-korea-university/COSE474' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241129_174356-5ni41cm3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b715df3-9636-4d07-8feb-5a9b0c5417eb",
   "metadata": {},
   "source": [
    "# Only CNN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "905d844d-4609-4fbd-bc6d-20b2afcf376c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/elicer/wandb/run-20241129_181452-qovqr858</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uus0314-korea-university/COSE474/runs/qovqr858' target=\"_blank\">Reduced_CNN_Only(Epoch 5)</a></strong> to <a href='https://wandb.ai/uus0314-korea-university/COSE474' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uus0314-korea-university/COSE474' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uus0314-korea-university/COSE474/runs/qovqr858' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474/runs/qovqr858</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.6147, Accuracy: 0.6167\n",
      "Epoch [2/5], Loss: 0.5515, Accuracy: 0.6917\n",
      "Epoch [3/5], Loss: 0.4775, Accuracy: 0.7383\n",
      "Epoch [4/5], Loss: 0.4157, Accuracy: 0.7900\n",
      "Epoch [5/5], Loss: 0.3501, Accuracy: 0.8417\n",
      "Validation Loss: 0.6822, Accuracy: 0.6200\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁▃▅▆█</td></tr><tr><td>Train Loss</td><td>█▂▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁</td></tr><tr><td>Validation Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>0.84167</td></tr><tr><td>Train Loss</td><td>0.35014</td></tr><tr><td>Validation Accuracy</td><td>0.62</td></tr><tr><td>Validation Loss</td><td>0.68222</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Reduced_CNN_Only(Epoch 5)</strong> at: <a href='https://wandb.ai/uus0314-korea-university/COSE474/runs/qovqr858' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474/runs/qovqr858</a><br/> View project at: <a href='https://wandb.ai/uus0314-korea-university/COSE474' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 150 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241129_181452-qovqr858/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from torchvision import transforms \n",
    "\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "class CNNOnly(nn.Module):\n",
    "    def __init__(self, num_classes=2, reduced_dim=1024):\n",
    "        super(CNNOnly, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.AdaptiveAvgPool2d((32, 32)),  # 출력 크기를 고정\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.feature_reduce = nn.Linear(64 * 32 * 32, reduced_dim)  # 실제 CNN 출력 크기로 변경\n",
    "        self.fc1 = nn.Linear(reduced_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_features = self.cnn(x)\n",
    "        cnn_features = self.feature_reduce(cnn_features)\n",
    "        x = self.fc1(cnn_features)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def train(model, train_loader, device, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_accuracy})\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_and_visualize(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(valid_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "\n",
    "            for img_idx in range(images.size(0)):\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.imshow(images[img_idx].cpu().squeeze(), cmap=\"gray\")\n",
    "                title = f\"GT: {labels[img_idx].item()}, Pred: {predicted[img_idx].item()}\"\n",
    "                ax.set_title(title)\n",
    "                ax.axis(\"off\")\n",
    "                wandb.log({f\"Sample Prediction (Batch {batch_idx}, Image {img_idx})\": wandb.Image(fig)})\n",
    "                plt.close(fig)\n",
    "\n",
    "    valid_loss = running_loss / len(valid_loader)\n",
    "    valid_accuracy = correct / total\n",
    "    wandb.log({\"Validation Loss\": valid_loss, \"Validation Accuracy\": valid_accuracy})\n",
    "    print(f\"Validation Loss: {valid_loss:.4f}, Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "wandb.init(project=\"COSE474\", name=\"Reduced_CNN_Only(Epoch 5)\")\n",
    "wandb.config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_epochs\": 5,\n",
    "    \"num_classes\": 2\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = MRIDataset(train_images, train_labels)\n",
    "valid_dataset = MRIDataset(valid_images, valid_labels)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNNOnly(num_classes=wandb.config[\"num_classes\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=wandb.config[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train(model, train_loader, device, optimizer, criterion, num_epochs=wandb.config[\"num_epochs\"])\n",
    "\n",
    "\n",
    "evaluate_and_visualize(model, valid_loader, device)\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17679c20-32c8-4412-9da3-b360fcf80ee9",
   "metadata": {},
   "source": [
    "# TDA Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f98ed9-20b8-4328-b713-e53ce5076b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/elicer/wandb/run-20241210_101031-pbqnrjro</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/uus0314-korea-university/COSE474/runs/pbqnrjro' target=\"_blank\">TDA_Only(Epoch 5)</a></strong> to <a href='https://wandb.ai/uus0314-korea-university/COSE474' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/uus0314-korea-university/COSE474' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/uus0314-korea-university/COSE474/runs/pbqnrjro' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474/runs/pbqnrjro</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7595, Accuracy: 0.5300\n",
      "Epoch [2/5], Loss: 0.6716, Accuracy: 0.5883\n",
      "Epoch [3/5], Loss: 0.6454, Accuracy: 0.6550\n",
      "Epoch [4/5], Loss: 0.6453, Accuracy: 0.6250\n",
      "Epoch [5/5], Loss: 0.6372, Accuracy: 0.6350\n",
      "Validation Loss: 0.7457, Accuracy: 0.5133\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁▄█▆▇</td></tr><tr><td>Train Loss</td><td>█▃▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁</td></tr><tr><td>Validation Loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>0.635</td></tr><tr><td>Train Loss</td><td>0.63722</td></tr><tr><td>Validation Accuracy</td><td>0.51333</td></tr><tr><td>Validation Loss</td><td>0.7457</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TDA_Only(Epoch 5)</strong> at: <a href='https://wandb.ai/uus0314-korea-university/COSE474/runs/pbqnrjro' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474/runs/pbqnrjro</a><br/> View project at: <a href='https://wandb.ai/uus0314-korea-university/COSE474' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 150 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241210_101031-pbqnrjro/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gudhi as gd\n",
    "from gudhi import representations\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "def compute_persistent_landscape(image_tensor, num_landscapes=2, resolution=100):\n",
    "    \"\"\"\n",
    "    이미지 텐서를 입력받아 Persistent Landscape를 계산.\n",
    "    \"\"\"\n",
    "    image_np = image_tensor.permute(1, 2, 0).numpy()\n",
    "    resized_image = cv2.resize(image_np, (128, 128), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "    try:\n",
    "\n",
    "        cubical_complex = gd.CubicalComplex(top_dimensional_cells=resized_image)\n",
    "        persistence_diagram = cubical_complex.persistence()\n",
    "        h0 = cubical_complex.persistence_intervals_in_dimension(0)\n",
    "        h1 = cubical_complex.persistence_intervals_in_dimension(1)\n",
    "\n",
    "\n",
    "        def remove_infinite_intervals(intervals):\n",
    "            return intervals[np.isfinite(intervals[:, 1])] if len(intervals) > 0 else intervals\n",
    "\n",
    "        h0 = remove_infinite_intervals(h0)\n",
    "        h1 = remove_infinite_intervals(h1)\n",
    "\n",
    "\n",
    "        landscape_generator = representations.Landscape(num_landscapes=num_landscapes, resolution=resolution)\n",
    "        filtered_h0 = np.array(h0) if len(h0) > 0 else np.empty((0, 2))\n",
    "        filtered_h1 = np.array(h1) if len(h1) > 0 else np.empty((0, 2))\n",
    "\n",
    "\n",
    "        landscape_h0 = landscape_generator.fit_transform([filtered_h0]).flatten() if len(filtered_h0) > 0 else np.zeros(num_landscapes * resolution)\n",
    "        landscape_h1 = landscape_generator.fit_transform([filtered_h1]).flatten() if len(filtered_h1) > 0 else np.zeros(num_landscapes * resolution)\n",
    "\n",
    "\n",
    "        combined_feature = np.concatenate([landscape_h0, landscape_h1])\n",
    "        return torch.from_numpy(combined_feature).float()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"TDA feature computation error: {e}\")\n",
    "        return torch.zeros(num_landscapes * resolution * 2)\n",
    "\n",
    "\n",
    "class MRIDataset(Dataset):\n",
    "    def __init__(self, images, labels, num_landscapes=2, resolution=100):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.num_landscapes = num_landscapes\n",
    "        self.resolution = resolution\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        tda_features = compute_persistent_landscape(image, num_landscapes=self.num_landscapes, resolution=self.resolution)\n",
    "        return image, label, tda_features\n",
    "\n",
    "\n",
    "class TDAOnlyModel(nn.Module):\n",
    "    def __init__(self, tda_feature_dim=200, num_classes=2):\n",
    "        super(TDAOnlyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(tda_feature_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, tda_features):\n",
    "        x = self.fc1(tda_features)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, train_loader, device, optimizer, criterion, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for images, labels, tda_features in train_loader:\n",
    "            images, labels, tda_features = images.to(device), labels.to(device), tda_features.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(tda_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = correct / total\n",
    "        wandb.log({\"Train Loss\": train_loss, \"Train Accuracy\": train_accuracy})\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "def evaluate_and_visualize(model, valid_loader, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels, tda_features) in enumerate(valid_loader):\n",
    "            images, labels, tda_features = images.to(device), labels.to(device), tda_features.to(device)\n",
    "            outputs = model(tda_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            for img_idx in range(images.size(0)):\n",
    "                fig, ax = plt.subplots()\n",
    "                ax.imshow(images[img_idx].cpu().squeeze(), cmap=\"gray\")\n",
    "                title = f\"GT: {labels[img_idx].item()}, Pred: {predicted[img_idx].item()}\"\n",
    "                ax.set_title(title)\n",
    "                ax.axis(\"off\")\n",
    "                wandb.log({f\"Sample Prediction (Batch {batch_idx}, Image {img_idx})\": wandb.Image(fig)})\n",
    "                plt.close(fig)\n",
    "\n",
    "    valid_loss = running_loss / len(valid_loader)\n",
    "    valid_accuracy = correct / total\n",
    "    wandb.log({\"Validation Loss\": valid_loss, \"Validation Accuracy\": valid_accuracy})\n",
    "    print(f\"Validation Loss: {valid_loss:.4f}, Accuracy: {valid_accuracy:.4f}\")\n",
    "\n",
    "\n",
    "wandb.init(project=\"COSE474\", name=\"TDA_Only(Epoch 5)\")\n",
    "wandb.config = {\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"num_epochs\": 5,\n",
    "    \"num_classes\": 2,\n",
    "    \"num_landscapes\": 2,\n",
    "    \"resolution\": 100\n",
    "}\n",
    "\n",
    "num_landscapes = wandb.config[\"num_landscapes\"]\n",
    "resolution = wandb.config[\"resolution\"]\n",
    "tda_feature_dim = num_landscapes * resolution * 2\n",
    "\n",
    "train_dataset = MRIDataset(train_images, train_labels, num_landscapes=num_landscapes, resolution=resolution)\n",
    "valid_dataset = MRIDataset(valid_images, valid_labels, num_landscapes=num_landscapes, resolution=resolution)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TDAOnlyModel(tda_feature_dim=tda_feature_dim, num_classes=wandb.config[\"num_classes\"]).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=wandb.config[\"learning_rate\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train(model, train_loader, device, optimizer, criterion, num_epochs=wandb.config[\"num_epochs\"])\n",
    "\n",
    "\n",
    "evaluate_and_visualize(model, valid_loader, device)\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20c5a1ec-76ce-4621-8518-8ab99fbf3f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>▁█</td></tr><tr><td>Train Loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy</td><td>0.58833</td></tr><tr><td>Train Loss</td><td>0.67157</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">TDA_Only(Epoch 10)</strong> at: <a href='https://wandb.ai/uus0314-korea-university/COSE474/runs/9gbssxsh' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474/runs/9gbssxsh</a><br/> View project at: <a href='https://wandb.ai/uus0314-korea-university/COSE474' target=\"_blank\">https://wandb.ai/uus0314-korea-university/COSE474</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241210_100502-9gbssxsh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
